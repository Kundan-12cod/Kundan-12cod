{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2782070-80e3-44c0-8dd3-f1aaccd96a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385a3993-8212-423c-9178-44fbf22e2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris,load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2fbc9-00d7-418d-9f3e-e6c707041cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f16cd0-9957-4f7b-a520-f4ecbaae8889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d6b688-504d-4e53-a34d-cbe4e0fd8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load sample dataset (Iris dataset)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Bagging Classifier with Decision Trees\n",
    "bagging_clf = BaggingClassifier(    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate accuracy\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7579b8f-91c7-4726-b4ad-5997cbe59f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918f4bc-b936-468d-97b3-5aa30ee50eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample dataset (Diabetes dataset)\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Bagging Regressor with Decision Trees\n",
    "bagging_reg = BaggingRegressor(\n",
    "    DecisionTreeRegressor(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate using MSE\n",
    "y_pred = bagging_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Bagging Regressor MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112058df-a401-4cc9-8781-2a7c7929fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4e887f-14bf-48a4-8921-f1f24cfdc8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Print feature importance scores\n",
    "importances = rf_clf.feature_importances_\n",
    "for feature, importance in zip(data.feature_names, importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78313d4-ef73-419b-a70f-bc1448e3681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#24. Train a Random Forest Regressor and compare its performance with a single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f84011-2e6a-4cab-987d-ecad094a4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single Decision Tree Regressor\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(X_train, y_train)\n",
    "y_pred_tree = tree_reg.predict(X_test)\n",
    "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "# Compare performance\n",
    "print(f\"Decision Tree Regressor MSE: {mse_tree:.2f}\")\n",
    "print(f\"Random Forest Regressor MSE: {mse_rf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71065b09-7bbe-4174-b900-f3217d5df060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#25.Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82effe2e-7e10-4061-8eb2-0fcaba637260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier with OOB score enabled\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Print OOB score\n",
    "print(f\"OOB Score: {rf_clf.oob_score_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5c7ae-8ad2-4b05-9a97-da88a76abe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#26. Train a Bagging Classifier using SVM as a base estimator and print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84507819-7c89-4a45-aa2a-3f0b93446dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bagging Classifier with SVM as base estimator\n",
    "bagging_clf = BaggingClassifier(\n",
    "    \n",
    "    estimator= SVC(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate accuracy\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Bagging Classifier with SVM Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c42255-324f-47d1-9261-926ccfbdc398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#27.  Train a Random Forest Classifier with different numbers of trees and compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ce0ae-7831-4007-b0ac-11b6a46e29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of trees\n",
    "n_trees = [10, 50, 100, 200]\n",
    "accuracies = []\n",
    "\n",
    "for n in n_trees:\n",
    "    rf_clf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(n_trees, accuracies, marker='o')\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Random Forest Accuracy vs Number of Trees\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781cb93-d4c2-44a2-8f1f-499967648642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dc5ab-bda4-40c8-8fe2-ecc47a1f5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset as an example\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "\n",
    "# Now you can proceed with your model training:\n",
    "# Train Bagging Classifier with Logistic Regression as base estimator\n",
    "bagging_clf = BaggingClassifier(estimator=LogisticRegression(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate AUC score\n",
    "y_pred_proba = bagging_clf.predict_proba(X_test)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"Bagging Classifier with Logistic Regression AUC: {auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5b0240-79a1-40d1-81b6-35041767fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#29.  Train a Random Forest Regressor and analyze feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491a5c5-cd59-4054-b164-57d7dde72cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Print feature importance scores\n",
    "importances = rf_reg.feature_importances_\n",
    "for feature, importance in zip(data.feature_names, importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b13c5-2617-4102-8eee-d559c8fd2108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#30.  Train an ensemble model using both Bagging and Random Forest and compare accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3d2b3-2c9c-4ed8-aa23-7ca3fd813f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Compare accuracy\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d7a7f-0de7-43d6-973d-f87e2ff2838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d22c0-5313-42a2-a177-4720e884bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and accuracy\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Accuracy: {grid_search.best_score_:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a284f-34fa-4dac-a4e5-704c17e26be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#32. Train a Bagging Regressor with different numbers of base estimators and compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac0d33a-3a6e-4fed-948b-d5966effa4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of base estimators\n",
    "n_estimators = [10, 50, 100, 200]\n",
    "mse_scores = []\n",
    "\n",
    "for n in n_estimators:\n",
    "    bagging_reg = BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(),\n",
    "        n_estimators=n,\n",
    "        random_state=42\n",
    "    )\n",
    "    bagging_reg.fit(X_train, y_train)\n",
    "    y_pred = bagging_reg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(n_estimators, mse_scores, marker='o')\n",
    "plt.xlabel(\"Number of Base Estimators\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Bagging Regressor Performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ceca5f-e8ba-44c2-adc9-ac0aec23c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#33. Random Forest Classifier and analyze misclassified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e38e85-a9e7-4880-8842-f3da07716c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and identify misclassified samples\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "misclassified = np.where(y_pred != y_test)[0]\n",
    "\n",
    "# Print misclassified samples\n",
    "print(f\"Misclassified Samples: {misclassified}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56fa9de-4f3c-4143-b051-aacd59488343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#34.Train a Bagging Classifier and compare its performance with a single Decision Tree Classifierabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2529d-bd58-4641-8913-b36392178a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a single Decision Tree Classifier\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "\n",
    "# Train Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_clf.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "# Compare performance\n",
    "print(f\"Decision Tree Classifier Accuracy: {accuracy_tree:.2f}\")\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d3baf6-9e99-4bee-b0d8-7577c66e8ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#35. Train a Random Forest Classifier and visualize the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd603f18-a095-4be0-8429-7573d92ad552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and create confusion matrix\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29bf33-0ae5-4de5-a389-967473730ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#36 Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f7ab4-6984-40e4-92d2-e6e43d6b7d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base estimators\n",
    "base_estimators = [\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('svm', SVC()),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "# Train Stacking Classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate accuracy\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Stacking Classifier Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d366a1-162f-4333-a38a-0857d952712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#37.Train a Random Forest Classifier and print the top 5 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5278dff3-c348-49ad-872f-a3b2056233a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_clf.feature_importances_\n",
    "feature_importance_pairs = list(zip(data.feature_names, importances))\n",
    "\n",
    "# Sort and print top 5 features\n",
    "top_5_features = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)[:5]\n",
    "for feature, importance in top_5_features:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad780ef4-aa72-4f15-b694-5a5f199edac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf73e9-b049-4284-9705-445aa85a5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f9f47-c418-473c-88b7-b35e9a754541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73cd48-8cb4-4ab6-be54-a3bd66e4ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different max_depth values\n",
    "max_depths = [None, 5, 10, 20, 30]\n",
    "accuracies = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(max_depths, accuracies, marker='o')\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Effect of Max Depth on Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ede83b-9c7a-4d2f-9cd7-2b129d724da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa45a12-1749-4a3c-9ae0-29c214dac6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bagging Regressor with Decision Tree\n",
    "bagging_reg_dt = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_reg_dt.fit(X_train, y_train)\n",
    "y_pred_dt = bagging_reg_dt.predict(X_test)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "\n",
    "# Train Bagging Regressor with KNeighbors\n",
    "bagging_reg_knn = BaggingRegressor(\n",
    "    estimator=KNeighborsRegressor(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_reg_knn.fit(X_train, y_train)\n",
    "y_pred_knn = bagging_reg_knn.predict(X_test)\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "\n",
    "# Compare performance\n",
    "print(f\"Bagging Regressor with Decision Tree MSE: {mse_dt:.2f}\")\n",
    "print(f\"Bagging Regressor with KNeighbors MSE: {mse_knn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00325c90-ad5e-4fd4-b48c-b7cd83ea89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9513d2-3ee2-42d6-834f-c4a0dffbd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and evaluate ROC-AUC\n",
    "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51652b82-e23c-4e7c-a82b-8692df3a3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#42.Train a Bagging Classifier and evaluate its performance using cross-validatio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ada77-0b97-45e6-a0d5-87e428153bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bagging Classifier\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Evaluate using cross-validation\n",
    "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02ece6-70c5-4f64-94e6-fae41f8c81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#43. Train a Random Forest Classifier and plot the Precision-Recall curv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce32c75-38cb-4b64-b1ae-4c5f5a70affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = rf_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Plot the curve\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415303dc-6ddc-4798-a176-9d988ae7f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#44 Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548aa70-e014-44d4-b6f9-8bad3030ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base estimators\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('lr', LogisticRegression())\n",
    "]\n",
    "\n",
    "# Train Stacking Classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
    "stacking_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e1cfb7-63d2-4c88-ac38-67ee9a7a257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3807d2-317b-4510-ad47-3a9901d476c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample dataset (Diabetes dataset)\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Test different levels of bootstrap samples\n",
    "bootstrap_samples = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]  # Fraction of the dataset to use for each bootstrap sample\n",
    "mse_scores = []\n",
    "\n",
    "for sample in bootstrap_samples:\n",
    "    # Train Bagging Regressor with different bootstrap sample sizes\n",
    "    bagging_reg = BaggingRegressor(\n",
    "        estimator=DecisionTreeRegressor(),\n",
    "        n_estimators=10,  # Number of base estimators\n",
    "        max_samples=sample,  # Fraction of the dataset to use for each bootstrap sample\n",
    "        random_state=42\n",
    "    )\n",
    "    bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate using Mean Squared Error (MSE)\n",
    "    y_pred = bagging_reg.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "    # Print results for each bootstrap sample size\n",
    "    print(f\"Bootstrap Sample Size: {sample}, MSE: {mse:.2f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(bootstrap_samples, mse_scores, marker='o')\n",
    "plt.xlabel(\"Bootstrap Sample Size\")\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "plt.title(\"Bagging Regressor Performance vs Bootstrap Sample Size\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43f81c-b8a1-4cfd-b95e-6d6bda91a053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
